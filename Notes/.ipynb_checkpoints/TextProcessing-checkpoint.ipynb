{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "PSY 394U <b>Data Analytics with Python</b>, Spring 2019\n",
    "\n",
    "\n",
    "<img style=\"width: 700px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2019/blob/master/Images/Banner.png?raw=true\" alt=\"title pics\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:40px; margin-bottom: 30px;\"><b> Text processing </b></p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:18px; margin-bottom: 32px;\"><b>April 16, 2019</b></p>\n",
    "\n",
    "<hr style=\"height:5px;border:none\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Example text data\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "To start out with **NLTK** (**Natural Language Toolkit**), we first download some example data available for NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<NLTKIntro.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# downloading example text corpora \"book\"\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should open a downloader window\n",
    "\n",
    "<img style=\"width: 600px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2019/blob/master/Images/nltk_downloader.png?raw=true\" alt=\"NLTK downloader\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the downloader, please download the collection **book**. It contains a number of corpora (plural of corpus) used in the [NLTK book](http://www.nltk.org/book/). A corpus is a collection of texts. It may take a few minutes to download this collection. It takes about 420MB of disk space on your computer. \n",
    "\n",
    "Now, let's take a look at the *Gutenberg corpus* (**`gutenberg`** under **`nltk.corpus`**). The *Gutenberg* corpus includes a number of literary works, and can be used as example data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the Gutenberg corpus\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the content of this corpus by the **`fileids()`** method, which produces a list of files as part of this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we will use Emma by Jane Austen (**`austen-emma.txt`**). We can simply load the raw text data with the **`raw`** method associated with the corpus `gutenberg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was t\n"
     ]
    }
   ],
   "source": [
    "# loading the raw text\n",
    "emmaRawText = gutenberg.raw('austen-emma.txt')\n",
    "print(emmaRawText[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can load as a collection of words using the **`words`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# loading words\n",
    "emmaWords = gutenberg.words('austen-emma.txt')\n",
    "print(emmaWords[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing sentences and words\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "NLTK comes with **tokenizers** that let up split a text data into sentences or words. Here is an example. For this example, we will be downloading a text data set (The Adventures of Sherlock Holmes by Arthur Conan Doyle) from the [Project Gutenberg's main web site](http://www.gutenberg.org), a repository of free electronic books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Tokenize.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Loading The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "# from the Project Gutenberg\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/ebooks/1661.txt.utf-8\"\n",
    "response = request.urlopen(url)\n",
    "rawText = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **`rawText`** contains the entire book. Here is an excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "ADVENTURE I. A SCANDAL IN BOHEMIA\r\n",
      "\r\n",
      "I.\r\n",
      "\r\n",
      "To Sherlock Holmes she is always THE woman. I have seldom heard\r\n",
      "him mention her under any other name. In his eyes she eclipses\r\n",
      "and predominates the whole of her sex. It was not that he felt\r\n",
      "any emotion akin to love for Irene Adler. All emotions, and that\r\n",
      "one particularly, were abhorrent to his cold, precise but\r\n",
      "admirably balanced mind. He was, I take it, the most perfect\r\n",
      "reasoning and observing machine that the world has seen, but as a\r\n",
      "lover he would have placed himself in a false position. He never\r\n",
      "spoke of the softer passions, \n"
     ]
    }
   ],
   "source": [
    "print(rawText[1210:1800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence tokenizer, **`sent_tokenize`** breaks up a text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To Sherlock Holmes she is always THE woman.', 'I have seldom heard\\r\\nhim mention her under any other name.', 'In his eyes she eclipses\\r\\nand predominates the whole of her sex.']\n"
     ]
    }
   ],
   "source": [
    "# breaking up the raw text into sentences\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "\n",
    "print(sentText[14:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word tokenizer, **`word_tokenize`** breaks up a text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman', '.']\n",
      "['I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']\n",
      "['In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.']\n"
     ]
    }
   ],
   "source": [
    "# breaking up sentences into words\n",
    "print(nltk.word_tokenize(sentText[14]))\n",
    "print(nltk.word_tokenize(sentText[15]))\n",
    "print(nltk.word_tokenize(sentText[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaking up the raw text into words\n",
    "wordText = nltk.word_tokenize(rawText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'she',\n",
       " 'is',\n",
       " 'always',\n",
       " 'THE',\n",
       " 'woman',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordText[220:235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency\n",
    "\n",
    "Now that the entire text is tokenized into words, we can determine word frequencies using the **`FreqDist`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "wordDist = nltk.FreqDist(wordText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the resulting **`wordDist`** is a dictionary-like object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 7779, '.': 5867, 'the': 5420, 'I': 3034, 'and': 2871, 'of': 2733, 'to': 2729, '``': 2723, 'a': 2595, \"''\": 2392, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of most frequent words with the **`most_common`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 7779), ('.', 5867), ('the', 5420), ('I', 3034), ('and', 2871), ('of', 2733), ('to', 2729), ('``', 2723), ('a', 2595), (\"''\", 2392), ('in', 1744), ('that', 1662), ('was', 1395), ('it', 1302), ('you', 1271), ('he', 1167), ('is', 1134), ('his', 1102), ('have', 907), ('my', 906), ('with', 849), ('had', 824), ('as', 780), ('which', 770), ('at', 743), ('?', 737), ('for', 716), ('not', 686), ('be', 642), ('me', 635)]\n"
     ]
    }
   ],
   "source": [
    "print(wordDist.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can get the frequency of a particular word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDist['adventure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDist['deduction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating string data\n",
    "\n",
    "After tokenization, the text data can be handled as a collection of words. We can take advantage of various string manipulation methods and functions available in Python. For example, we can convert all words to lower case with the **`lower()`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 7779), ('.', 5867), ('the', 5793), ('and', 3061), ('i', 3034), ('of', 2777), ('to', 2761), ('``', 2723), ('a', 2697), (\"''\", 2392), ('in', 1818), ('that', 1757), ('it', 1736), ('you', 1536), ('he', 1484), ('was', 1413), ('his', 1158), ('is', 1148), ('my', 999), ('have', 927), ('with', 877), ('as', 861), ('had', 833), ('at', 782), ('which', 776), ('for', 749), ('?', 737), ('not', 709), ('but', 648), ('be', 646)]\n"
     ]
    }
   ],
   "source": [
    "# word frequency after converting to lower case\n",
    "wordTextLower = [w.lower() for w in wordText]\n",
    "wordDistLower = nltk.FreqDist(wordTextLower)\n",
    "print(wordDistLower.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a list of unique words in a word list by the **`set`** function. Here, we can extract some words satisfying certain conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'breckinridge\", \"'certainly\", \"'encyclopaedia\", \"'gesellschaft\", \"'hampshire\", \"'photography\", \"'pondicherry\", \"'precisely\", \"'remarkable\", \"'undoubtedly\"]\n"
     ]
    }
   ],
   "source": [
    "# just long words (10 characters or more)\n",
    "wordSetLower = sorted(set(wordTextLower))  # unique word list\n",
    "longWords = [w for w in wordSetLower if len(w)>9]\n",
    "print(longWords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely', 'considerable', 'electronic', 'foundation', 'gutenberg-tm', 'interesting', 'photograph', 'stepfather', 'understand']\n"
     ]
    }
   ],
   "source": [
    "# long words appearing more than 20 times\n",
    "longFreqWords = [w for w in wordSetLower\n",
    "                 if (len(w)>9) and wordDistLower[w]>20]\n",
    "print(longFreqWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **Long, frequent words, Emma**. Generate a list of long words (10 characters or more) appearing frequently (more than 20 times) in Emma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Part-of-speech tags\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "**Part-of-speech** (**POS**) tags indicate categories of words with similar grammatical properties (e.g., verbs, nouns, adjectives, etc.). In NLTK, the **`pos_tag`** function can assign POS tags for each word. Here is an example. We focus on a sentence from Emma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<POS-Tag.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness of her temper had hardly allowed\n",
      "her to impose any restraint; and the shadow of authority being\n",
      "now long passed away, they had been living together as friend and\n",
      "friend very mutually attached, and Emma doing just what she liked;\n",
      "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
      "her own.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "emmaRawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "emmaSents = nltk.sent_tokenize(emmaRawText)\n",
    "print(emmaSents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break this up into words via `word_tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmaWords = nltk.word_tokenize(emmaSents[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Even',\n",
       " 'before',\n",
       " 'Miss',\n",
       " 'Taylor',\n",
       " 'had',\n",
       " 'ceased',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'the',\n",
       " 'nominal',\n",
       " 'office',\n",
       " 'of',\n",
       " 'governess',\n",
       " ',',\n",
       " 'the',\n",
       " 'mildness',\n",
       " 'of',\n",
       " 'her',\n",
       " 'temper',\n",
       " 'had',\n",
       " 'hardly',\n",
       " 'allowed',\n",
       " 'her',\n",
       " 'to',\n",
       " 'impose',\n",
       " 'any',\n",
       " 'restraint',\n",
       " ';',\n",
       " 'and',\n",
       " 'the',\n",
       " 'shadow',\n",
       " 'of',\n",
       " 'authority',\n",
       " 'being',\n",
       " 'now',\n",
       " 'long',\n",
       " 'passed',\n",
       " 'away',\n",
       " ',',\n",
       " 'they',\n",
       " 'had',\n",
       " 'been',\n",
       " 'living',\n",
       " 'together',\n",
       " 'as',\n",
       " 'friend',\n",
       " 'and',\n",
       " 'friend',\n",
       " 'very',\n",
       " 'mutually',\n",
       " 'attached',\n",
       " ',',\n",
       " 'and',\n",
       " 'Emma',\n",
       " 'doing',\n",
       " 'just',\n",
       " 'what',\n",
       " 'she',\n",
       " 'liked',\n",
       " ';',\n",
       " 'highly',\n",
       " 'esteeming',\n",
       " 'Miss',\n",
       " 'Taylor',\n",
       " \"'s\",\n",
       " 'judgment',\n",
       " ',',\n",
       " 'but',\n",
       " 'directed',\n",
       " 'chiefly',\n",
       " 'by',\n",
       " 'her',\n",
       " 'own',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmaWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we assign POS tags to this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Even', 'RB'), ('before', 'IN'), ('Miss', 'NNP'), ('Taylor', 'NNP'), ('had', 'VBD'), ('ceased', 'VBN'), ('to', 'TO'), ('hold', 'VB'), ('the', 'DT'), ('nominal', 'JJ'), ('office', 'NN'), ('of', 'IN'), ('governess', 'NN'), (',', ','), ('the', 'DT'), ('mildness', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('temper', 'NN'), ('had', 'VBD'), ('hardly', 'RB'), ('allowed', 'VBN'), ('her', 'PRP'), ('to', 'TO'), ('impose', 'VB'), ('any', 'DT'), ('restraint', 'NN'), (';', ':'), ('and', 'CC'), ('the', 'DT'), ('shadow', 'NN'), ('of', 'IN'), ('authority', 'NN'), ('being', 'VBG'), ('now', 'RB'), ('long', 'RB'), ('passed', 'VBN'), ('away', 'RB'), (',', ','), ('they', 'PRP'), ('had', 'VBD'), ('been', 'VBN'), ('living', 'VBG'), ('together', 'RB'), ('as', 'IN'), ('friend', 'NN'), ('and', 'CC'), ('friend', 'VB'), ('very', 'RB'), ('mutually', 'RB'), ('attached', 'VBN'), (',', ','), ('and', 'CC'), ('Emma', 'NNP'), ('doing', 'VBG'), ('just', 'RB'), ('what', 'WP'), ('she', 'PRP'), ('liked', 'VBD'), (';', ':'), ('highly', 'RB'), ('esteeming', 'VBG'), ('Miss', 'NNP'), ('Taylor', 'NNP'), (\"'s\", 'POS'), ('judgment', 'NN'), (',', ','), ('but', 'CC'), ('directed', 'VBD'), ('chiefly', 'NN'), ('by', 'IN'), ('her', 'PRP$'), ('own', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS tagging of an example sentence\n",
    "emmaTagged = nltk.pos_tag(emmaWords)\n",
    "print(emmaTagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is represented as a tuple, a pair of the word and its POS tag. You may ask, \"what are those symbols, such as RB or IN?\" You can see a list of tags by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these tags, we can easily extract a certain category of words. For example, a tag for a verb usually starts with characters **`\"VB\"`**. So we can extract verbs only from the text data by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('had', 'VBD'), ('ceased', 'VBN'), ('hold', 'VB'), ('had', 'VBD'), ('allowed', 'VBN'), ('impose', 'VB'), ('being', 'VBG'), ('passed', 'VBN'), ('had', 'VBD'), ('been', 'VBN'), ('living', 'VBG'), ('friend', 'VB'), ('attached', 'VBN'), ('doing', 'VBG'), ('liked', 'VBD'), ('esteeming', 'VBG'), ('directed', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "# extracting verbs only (starting with VB)\n",
    "emmaVerbs = [w for w in emmaTagged if 'VB' in w[1]]\n",
    "print(emmaVerbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or adverbs (tags starting with **`\"RB\"`**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Even', 'RB'), ('hardly', 'RB'), ('now', 'RB'), ('long', 'RB'), ('away', 'RB'), ('together', 'RB'), ('very', 'RB'), ('mutually', 'RB'), ('just', 'RB'), ('highly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# extracting adverbs only\n",
    "emmaAdv = [w for w in emmaTagged if 'RB' in w[1]]\n",
    "print(emmaAdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or proper nouns (tags starting with **`\"NNP\"`**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Miss', 'NNP'), ('Taylor', 'NNP'), ('Emma', 'NNP'), ('Miss', 'NNP'), ('Taylor', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# extracting proper nouns only\n",
    "emmaNNP = [w for w in emmaTagged if 'NNP' in w[1]]\n",
    "print(emmaNNP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **POS frequencies**. In the POS-tagged sentence **`emmaTagged`**, count the number of\n",
    "  * Verbs: tags starting with **VB**\n",
    "  * Nouns: tags starting with **NN**\n",
    "  * Adjectives: tags starting with **JJ**\n",
    "  * Adverbs: tags starting with **RB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stop words and punctuation marks\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "In the word frequency from an earlier example, you may have noticed that the most frequent words are actually punctuation marks (e.g., commas (,), periods (.), question marks (?), etc.) and a class of words known as stop words (e.g., \"the\", \"and\", \"of\", \"to\"). When analyzing text data, punctuation marks and stop words do not provide much information. Thus, we can eliminate these words.\n",
    "\n",
    "## Removing punctuation marks\n",
    "\n",
    "We can remove punctuation marks simply by using the **`isalpha()`** method for string data. For example, in the example sentence from Emma we saw earlier,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<StopwordsPunct.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness of her temper had hardly allowed\n",
      "her to impose any restraint; and the shadow of authority being\n",
      "now long passed away, they had been living together as friend and\n",
      "friend very mutually attached, and Emma doing just what she liked;\n",
      "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
      "her own.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "rawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "print(sentText[5])\n",
    "wordText = nltk.word_tokenize(sentText[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine whether each word consists of alphabets. If a word is anything other than alphabets, it will be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation marks, making all words lower case\n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'before', 'miss', 'taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', 'the', 'mildness', 'of', 'her', 'temper', 'had', 'hardly', 'allowed', 'her', 'to', 'impose', 'any', 'restraint', 'and', 'the', 'shadow', 'of', 'authority', 'being', 'now', 'long', 'passed', 'away', 'they', 'had', 'been', 'living', 'together', 'as', 'friend', 'and', 'friend', 'very', 'mutually', 'attached', 'and', 'emma', 'doing', 'just', 'what', 'she', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'but', 'directed', 'chiefly', 'by', 'her', 'own']\n"
     ]
    }
   ],
   "source": [
    "print(wordDePunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Now, to remove stop words, we first have to import the corpus of stop words **`stopwords`**, and select that of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))  # stop words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we examine whether each word is part of the set of stop words (**`stop_words`**). If a word is a stop word, that word is eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', 'shadow', 'authority', 'long', 'passed', 'away', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', 'emma', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'directed', 'chiefly']\n"
     ]
    }
   ],
   "source": [
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]\n",
    "print(wordNoStopwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency, before and after cleaning\n",
    "\n",
    "We shall see how removing punctuation marks and stop words alters the word frequency using the Sherlock Holmes example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<StopwordsFreq.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Loading The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "# from the Project Gutenberg\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/ebooks/1661.txt.utf-8\"\n",
    "response = request.urlopen(url)\n",
    "rawText = response.read().decode('utf8')\n",
    "\n",
    "# tokenizing\n",
    "wordText = nltk.word_tokenize(rawText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of top 30 most frequent words, before the removal of punctuation marks and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before text processing\n",
      ",              \t  7779\n",
      ".              \t  5867\n",
      "the            \t  5420\n",
      "I              \t  3034\n",
      "and            \t  2871\n",
      "of             \t  2733\n",
      "to             \t  2729\n",
      "``             \t  2723\n",
      "a              \t  2595\n",
      "''             \t  2392\n",
      "in             \t  1744\n",
      "that           \t  1662\n",
      "was            \t  1395\n",
      "it             \t  1302\n",
      "you            \t  1271\n",
      "he             \t  1167\n",
      "is             \t  1134\n",
      "his            \t  1102\n",
      "have           \t   907\n",
      "my             \t   906\n",
      "with           \t   849\n",
      "had            \t   824\n",
      "as             \t   780\n",
      "which          \t   770\n",
      "at             \t   743\n",
      "?              \t   737\n",
      "for            \t   716\n",
      "not            \t   686\n",
      "be             \t   642\n",
      "me             \t   635\n"
     ]
    }
   ],
   "source": [
    "# word frequency before removing punctuations and stop words\n",
    "print('Before text processing')\n",
    "wordFreqBefore = nltk.FreqDist(wordText)\n",
    "for iWord in wordFreqBefore.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a list of 30 most frequent words AFTER removing punctuation marks and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After text processing\n",
      "said           \t   486\n",
      "upon           \t   467\n",
      "holmes         \t   466\n",
      "one            \t   374\n",
      "would          \t   333\n",
      "man            \t   303\n",
      "could          \t   288\n",
      "little         \t   269\n",
      "see            \t   232\n",
      "may            \t   210\n",
      "us             \t   184\n",
      "well           \t   176\n",
      "think          \t   174\n",
      "must           \t   171\n",
      "know           \t   171\n",
      "shall          \t   171\n",
      "come           \t   160\n",
      "time           \t   151\n",
      "came           \t   146\n",
      "two            \t   143\n",
      "door           \t   141\n",
      "back           \t   139\n",
      "room           \t   134\n",
      "face           \t   128\n",
      "might          \t   126\n",
      "matter         \t   125\n",
      "much           \t   121\n",
      "way            \t   116\n",
      "yes            \t   114\n",
      "heard          \t   113\n"
     ]
    }
   ],
   "source": [
    "# removing punctuations and stopwords\n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]\n",
    "\n",
    "\n",
    "# word frequency after removing punctuations and stop words\n",
    "print('After text processing')\n",
    "wordFreqAfter = nltk.FreqDist(wordNoStopwd)\n",
    "for iWord in wordFreqAfter.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stemming and lemmatizing\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "Consider the word \"spam.\" It can be a noun in the singular or plural form (\"spam\" or \"spams\"). It can be a verb, thus conjugates depending on the context (e.g., \"spamming\", \"spammed\"). Or a new word can be created by adding a suffix (e.g., \"spammer\", \"spamize\", \"spamly\"). In terms of semantics, these are the same word, or closely related words. However, because they are spelled differently, they may be considered as distinct words. Two ways to get around that problem are **stemming** and **lemmatizing**.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "**Stemming** extracts the root from a word. Here is a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Stemming.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sample words\n",
    "sampleWords = ['spam', 'spams', 'spamming', 'spammed', 'spammer', 'spammers',\n",
    "               'spamize','spamly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform stemming, we need to use the **`PorterStemmer`** transformation object under **`nltk.stem`**. We first need to define a stemming transformation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer object\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the object **`ps`** is defined, then we can use the **`stem`** method to determine the stem of a word. In our **`sampleWords`**, after stemming,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n",
      "spam\n",
      "spam\n",
      "spam\n",
      "spammer\n",
      "spammer\n",
      "spamiz\n",
      "spamli\n"
     ]
    }
   ],
   "source": [
    "for w in sampleWords:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are some common stems (**`spam`** and **`spammer`**). However, the other stems do not resemble real words (**`spamiz`** and **`spamli`**).\n",
    "\n",
    "Just for fun, we can apply stemming to our example sentence from Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "rawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "wordText = nltk.word_tokenize(sentText[5])\n",
    "\n",
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the words before stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', 'shadow', 'authority', 'long', 'passed', 'away', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', 'emma', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'directed', 'chiefly']\n"
     ]
    }
   ],
   "source": [
    "# before stemming\n",
    "print(wordNoStopwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the words after stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceas', 'hold', 'nomin', 'offic', 'gover', 'mild', 'temper', 'hardli', 'allow', 'impos', 'restraint', 'shadow', 'author', 'long', 'pass', 'away', 'live', 'togeth', 'friend', 'friend', 'mutual', 'attach', 'emma', 'like', 'highli', 'esteem', 'miss', 'taylor', 'judgment', 'direct', 'chiefli']\n"
     ]
    }
   ],
   "source": [
    "# after stemming\n",
    "wordStem = [ps.stem(w) for w in wordNoStopwd]\n",
    "print(wordStem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it works in some cases (e.g., `allowed` $\\rightarrow$ `allow` or `mutually` $\\rightarrow$ `mutual`). But some results are non-words (e.g., `ceased` $\\rightarrow$ `ceas` or `together` $\\rightarrow$ `togeth`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Unlike stemming, **lemmatizing** maps a word to its original form. For example,\n",
    "  * `cats` $\\rightarrow$ `cat`\n",
    "  * `cacti` $\\rightarrow$ `cactus`\n",
    "  * `geese` $\\rightarrow$ `goose`\n",
    "\n",
    "In NLTK, there is a transformation object **`WordNetLemmatizer`** under **`nltk.stem`**, that implements lemmatization. Here are some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Lemmatizing.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sample words\n",
    "sampleWords = ['cats','cacti','geese','rocks','oxen','ran','spamming',\n",
    "               'spammed','spammer','moves','movement','better']\n",
    "for w in sampleWords:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer object\n",
    "lmt = WordNetLemmatizer()\n",
    "\n",
    "# lemmatized words\n",
    "for w in sampleWords:\n",
    "    print(lmt.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the lemmatizer assumes that the input word is a noun. If the word is not a noun, you can specify that with the **`pos`** parameter (**`'a'`** for adjectives, **`'v'`** for verbs, and **`'r'`** for adverbs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some non-noun words\n",
    "print(lmt.lemmatize('ran', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lmt.lemmatize('better', pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to our sample sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "rawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "wordText = nltk.word_tokenize(sentText[5])\n",
    "\n",
    "# removing punctuation marks & stop words, making all words lower case\n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the words before lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before lemmatizing\n",
    "print(wordNoStopwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after lemmatizing\n",
    "wordLemma = [lmt.lemmatize(w) for w in wordNoStopwd]\n",
    "print(wordLemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, verbs are still not lemmatized. So we use information from POS tags to convert words according to their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using POS tags\n",
    "wordPOS = nltk.pos_tag(wordText)\n",
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordPOSDePunct = [(w[0].lower(), w[1]) for w in wordPOS if w[0].isalpha()]\n",
    "wordPOSNoStopwd = [w for w in wordPOSDePunct if w[0] not in stop_words]\n",
    "# initializing the lammatized word list\n",
    "wordPOSLemma = []\n",
    "for wPair in wordPOSNoStopwd:\n",
    "    if wPair[1][0] == 'J':   # i.e., adjectives\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='a'))\n",
    "    elif wPair[1][0] == 'V':  # i.e., verbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='v'))\n",
    "    elif 'RB' in wPair[1]:  # i.e., adverbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='r'))\n",
    "    else:\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0]))\n",
    "print(wordPOSLemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing and word frequency\n",
    "\n",
    "We can apply lemmatization on a larger text data. Let's examine the Sherlock Holmes example, and see how lemmatization affects word frequency counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<LemmatizingFreq.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Loading The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "# from the Project Gutenberg\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/ebooks/1661.txt.utf-8\"\n",
    "response = request.urlopen(url)\n",
    "rawText = response.read().decode('utf8')\n",
    "\n",
    "\n",
    "# tokenizing\n",
    "wordText = nltk.word_tokenize(rawText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 30 most frequent words, before lemmatization and other fancy text processing (removing punctuation marks and stop words, and turning all letters lower case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency before doing fancy text processing stuff\n",
    "print('Before text processing')\n",
    "wordFreqBefore = nltk.FreqDist(wordText)\n",
    "for iWord in wordFreqBefore.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some fancy text processing and lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]\n",
    "\n",
    "# Lemmatizing using POS tags\n",
    "lmt = WordNetLemmatizer()\n",
    "wordPOS = nltk.pos_tag(wordText)\n",
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordPOSDePunct = [(w[0].lower(), w[1]) for w in wordPOS if w[0].isalpha()]\n",
    "wordPOSNoStopwd = [w for w in wordPOSDePunct if w[0] not in stop_words]\n",
    "# initializing the lammatized word list\n",
    "wordPOSLemma = []\n",
    "for wPair in wordPOSNoStopwd:\n",
    "    if wPair[1][0] == 'J':   # i.e., adjectives\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='a'))\n",
    "    elif wPair[1][0] == 'V':  # i.e., verbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='v'))\n",
    "    elif 'RB' in wPair[1]:  # i.e., adverbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='r'))\n",
    "    else:\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall see how these steps affect the word frequency count. Here are the top 30 words on the processed and lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency after fancy text processing stuff\n",
    "print('After text processing')\n",
    "wordFreqAfter = nltk.FreqDist(wordPOSLemma)\n",
    "for iWord in wordFreqAfter.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **Backward lemmatized word frequency**. Words that can be lemmatized to **`think`** include:\n",
    "```\n",
    "think, Think, thinks, Thinks, thought, Thought, thinking, Thinking\n",
    "```\n",
    "  Determine the frequency counts of these words in the Sherlock Holmes data set before text processing (i.e., in **`wordFreqBefore`**). What is the sum of the frequencies?\n",
    "\n",
    "2. **Verb or noun?** Among the possible words listed in the previous exercise, word **`thought`** is lemmatized to **`think`** if it is a verb, while it is lemmatized to **`thought`** if it is a noun. Determine how many of **`thought`**'s are verbs and how many of them are nouns. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
