{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "PSY 394U <b>Data Analytics with Python</b>, Spring 2018\n",
    "\n",
    "\n",
    "<img style=\"width: 400px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/images/Title_pics.png?raw=true\" alt=\"title pics\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:40px; margin-bottom: 30px;\"><b> Decision trees </b></p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:18px; margin-bottom: 32px;\"><b>February 27, 2018</b></p>\n",
    "\n",
    "<hr style=\"height:5px;border:none\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is a decision tree?\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "## Let's divide and conquer\n",
    "\n",
    "You have a data set of wild mushrooms. They are labeled as either edible or poisonous. Along with this label of toxicity, there are three other features associated with mushrooms. Here is the data in a table form.\n",
    "\n",
    "<img style=\"width: 400px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/images/Tree_MushroomData.png?raw=true\" alt=\"Mushroom data table\"/>\n",
    "\n",
    "The features are:\n",
    "\n",
    "  * **`CapSurface`**: Cap surface\n",
    "     * 0: fibrous\n",
    "     * 1: scaly\n",
    "     * 2: smooth\n",
    "  * **`CapColor`**: Cap color\n",
    "     * 0: brown\n",
    "     * 1: white\n",
    "     * 2: yellow\n",
    "  * **`GillSize`**: Gill size\n",
    "     * 0: narrow\n",
    "     * 1: wide\n",
    "\n",
    "As you can see, the features are categorical. The target is:\n",
    "   \n",
    "  * **`Edible`**: Edible or poisonous?\n",
    "     * 0: edible\n",
    "     * 1: poisonous\n",
    "\n",
    "\n",
    "Your goal is to see if any of the features can split (or classify) the observations into **edible** and **poisonous**. Let's see how each of these features can split observations into two classes.\n",
    "\n",
    "<table style=\"width: 700px\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <img style=\"width: 233px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/graphviz/Tree_CapSurface.png?raw=true\" alt=\"CapSurface\"/>\n",
    "    </td>\n",
    "    <td>\n",
    "    <img style=\"width: 233px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/graphviz/Tree_CapColor.png?raw=true\" alt=\"CapColor\"/>\n",
    "    </td>\n",
    "    <td>\n",
    "    <img style=\"width: 233px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/graphviz/Tree_GillSize.png?raw=true\" alt=\"GillSize\"/>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Now which feature produced the best split? In order to determine that, we will use a metric called **entropy**. Here, the entropy we use is different from the entropy you learned in your physics class. **Entropy** is a measure of information content proposed by Claude Shannon in 1948. The entropy in a classification problem can be measured by how heterogeneous target classes are. For example, if there are an equal number of 0s and 1s in the target, then the entropy is high since the sample is highly heterogeneous. On the other hand, if the sample included only 0s or only 1s, then the resulting entropy is zero, since the data is homogeneous. To determine which features split the data the best, we look at whether a split resulted in reduced entropy, and if so, how much. In the example above, splitting the data based on **`GillSize`** seems to reduce the entropy the most. So we will choose to split the data by `GillSize` first. \n",
    "\n",
    "But even after splitting the data by `GillSize`, perhaps we can use another feature to split the resulting sub-sample further. So we will select another feature that reduces the entropy the most. In fact, we repeat this until the data is split into solely *edibles* or *poisonouses*. \n",
    "\n",
    "<img style=\"width: 350px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/graphviz/Tree_All.png?raw=true\" alt=\"Full decision tree\"/>\n",
    "\n",
    "The end result is known as a **decision tree**. It can be used to classify data based on how observations are split according to features. In general, a decision tree has the following form. \n",
    "\n",
    "<img style=\"width: 500px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/graphviz/SampleDecisionTree.png?raw=true\" alt=\"Sample decision tree\"/>\n",
    "\n",
    "The **root node** is the first feature used to split the data. **Decision nodes** are the subsequent splits based on other features. Finally, at **leaves** or **terminal nodes**, no further splitting takes place. Ideally a **leaf** contains only observations belonging to one target class (referred as **pure**). However, in reality, leaf nodes may contain multiple target classes. If that is the case, the majority vote in a leaf node determines the final classification of that node.\n",
    "\n",
    "\n",
    "## Decision trees: pros and cons\n",
    "\n",
    "Decision trees have some properties that are beneficial in some situations. However, there are some shortcomings as well. Here are some pros and cons of decision tree classifiers.\n",
    "\n",
    "### Pros:\n",
    "  * **Ability to handle both categorical and continuous features**. As you saw in our earlier example, a decision tree can handle categorical features. It can also handle continuous features. A continuous feature is dichotomized or binarized at a certain threshold (e.g., $X<1.75$) so that it becomes a categorical feature. The optimal threshold for dichotomization is determined so that entropy is minimized.\n",
    "  * **Interpretability**. A decision tree consists of a series of feature criteria that are intuitive and easy to understand. A decision tree can be visualized (as seen in some examples above), although it may involve additional software tools.\n",
    "  \n",
    "### Cons:\n",
    "  * **Tendency to over-fit data**. It is easy to produce a decision tree that fits the training data perfectly. However, such a tree often includes a large number of leaf nodes with one or two observations in one target class. Needless to say such a tree does not perform well as a classifier. Thus, some constraints are required when we generate a decision tree.\n",
    "  * **Biased to dominant target class**. Say there are 50 poisonous mushrooms and 1000 edible mushrooms in the mushroom data set. Then the decision tree classifier resulting from such a data set is trained to handle the edible class observations.\n",
    "  * **Impossible to find the optimal tree**. Depending on features, there are infinitely many possible decision trees constructed to handle the same data. Thus, determining the optimal tree is mathematically impossible. Decision trees we use are not the *best* solutions, but *good enough* solution to classify data.\n",
    "  * **Decision boundaries are rectangular, parallel to axes**. This is pertinent in continuous features. Since such features are dichotomized, the resulting decision boundaries are rectangular.\n",
    "\n",
    "## Drawing a decision tree\n",
    "\n",
    "Drawing a decision tree involves an external software package, and beyond the scope of this course. The interested students can read how to draw a decision tree from Scikit-learn's documentation: http://scikit-learn.org/stable/modules/tree.html#classification .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Example: iris data\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "Let's examine the iris data using a decision tree classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<IrisTree.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Loading data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data, with testing data comprising 40% of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spliting the data into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
    "                                                    random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier can be found as a the **`DecisionTreeClassifier`** transformation object under **`sklearn.tree`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy', \n",
    "                            min_samples_leaf = 3,\n",
    "                            max_depth = 4,\n",
    "                            random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the parameter **`criterion='entropy'`** to denote that we want to minimize entropy. The parameters **`min_samples_leaf`** and **`max_depth`** are used to avoid over-fitting the data. **`min_samples_leaf = 3`** means that a leaf has to contain at least 3 observations. **`max_depth = 4`** means that the tree can have at most 4 levels, starting from the root all the way down to the deepest leaf. We can also define **`random_state=0`** so that we can re-create the same decision tree again. Once the `DecisionTreeClassifier` object is defined, we can use the **`.fit`** method to learn from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the classification results on the testing data. Here, we can use **`.predict`** method to generate predicted class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  1 16]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        24\n",
      " versicolor       0.95      1.00      0.97        19\n",
      "  virginica       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.98      0.98      0.98        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification on the testing data set\n",
    "y_pred = dt.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it did a great job. For your information, here is the resulting decision tree, visualized.\n",
    "\n",
    "<img style=\"width: 500px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/graphviz/IrisDecisionTree.png?raw=true\" alt=\"Iris data decision tree\"/>\n",
    "\n",
    "Just to demonstrate the tendency for a decision tree to over-fit data, let's try the same analysis with an unconstrained decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  3 14]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        24\n",
      " versicolor       0.86      1.00      0.93        19\n",
      "  virginica       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.96      0.95      0.95        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decision tree classifier, unconstrained\n",
    "dtUnc = DecisionTreeClassifier(criterion='entropy',\n",
    "                               random_state=0)\n",
    "dtUnc.fit(X_train,y_train)\n",
    "y_pred_unc = dtUnc.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred_unc))\n",
    "print(classification_report(y_test, y_pred_unc,\n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are actually worse, possibly due to over-fitting. And here is what the unconstrained tree looks like. \n",
    "\n",
    "<img style=\"width: 600px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/graphviz/IrisDecisionTreeUnc.png?raw=true\" alt=\"Iris data decision tree, unconstrained\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An earlier classification tree indicated that the petal width and length may be the only features used to classify the data reliably. So we will re-construct the decision tree with those features only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<IrisTreePetalOnly.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = np.min(x,axis=0) - 1, np.max(x,axis=0) + 1\n",
    "    y_min, y_max = np.min(y,axis=0) - 1, np.max(y,axis=0) + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, x, y, h=.02, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    xx, yy = make_meshgrid(x, y, h)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "# Loading data\n",
    "iris = load_iris()\n",
    "X = iris.data[:,2:] # focusing on petal features only \n",
    "y = iris.target\n",
    "feature_names = iris.feature_names[2:]\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into the training and testing data, then fitting a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spliting the data into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
    "                                                    random_state=2018)\n",
    "\n",
    "\n",
    "# decision tree classifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy', \n",
    "                            min_samples_leaf = 3,\n",
    "                            max_depth = 4,\n",
    "                            random_state=0)\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's examine the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAGSCAYAAAAfJ52fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmcXXV9//HXe5ashCQkgBAQZBEB\nRaiRulG1agWrYlttQeve0sW1tj+rtrW1q3bRtm4UBQXEhUUUJAgJm+wQYljCZgiEZJKQPbNlZu6d\n+/n9cc6d3JncmTmznntn3s/H4z5y7znnfs/nJnA/9/P9fs/3KCIwMzPLoiHvAMzMrH44aZiZWWZO\nGmZmlpmThpmZZeakYWZmmTlpmJlZZk4aNmVJul7S+zMc1y7pmAk4/zOS3jje7Y4ijn+Q9L0xvP98\nSX83njFZ/WrKOwCb3iQ9AxwKFIFe4FHgEuCCiCiNpe2IOCvjcQeM5TxTXUT8ad4xWO1wpWG14G0R\nMQ84Cvgi8NfAhfmGZACSGvOOwWqLk4bVjIjYExHXAH8AvF/SiwEkzZT0n5KelfRc2l0yu/w+SWdL\nWi2pVdJTks5Mt98q6Y/S58dJuk3SHknbJf2o4v0h6bj0+XxJl0jaJmm9pL+V1JDu+4CkO9JYdkl6\nWtJw1czLJT2aHv8dSbMqzvvHktZK2inpGkmHp9uPTmNqqji28rMMGYekF6SftU3ScmBxZUCSrpC0\nJf27+IWkkyv2fVfSNyUtk9QBvD7d9s8Vx7w1/fveLekuSadU7PtrSS3puZ+Q9IZh/n6szjhpWM2J\niPuAjcAZ6aYvAi8ETgWOA5YAnweQdDpJd9b/AxYAvwE8U6XZfwJuBBYCRwBfHeT0XwXmA8cArwXe\nB3ywYv+vA0+QfBH/O3ChJA3xcd4DvBk4Nv0Mf5vG/ZvAvwG/DxwGrAd+OEQ7Aw0Vx/eBB9J9/wQM\nHNe5HjgeOARYBVw2YP+7gX8B5gF3VO6QdBpwEfAnwCLg/4Br0sR+AvBR4OVp5fhmqv9bWB1z0rBa\ntQk4KP0iPA/4i4jYGRFtwL8C56THfRi4KCKWR0QpIloi4vEq7RVIur8Oj4iuiLhj4AFpV8w5wGcj\noi0ingH+C3hvxWHrI+JbEdELXEzyhX/oEJ/jaxGxISJ2knwRn5tuf08a96qI6AY+C7xS0tHD/cUM\nFYek5wMvB/4uIroj4hfAtZVvjIiL0s/XDfwD8FJJ8ysO+WlE3Jn+fXYNOO95wP9FxL0R0RsRFwPd\nwCtIxqRmAidJao6IZyLiqYyfx+qEk4bVqiXATuBgYA7wQNodshv4ebod4EggyxfTpwEB90laI+lD\nVY5ZDDST/OovW5/GUral/CQiOtOnQw2kbxjQ1uHp88MrzxMR7cCOAecaymBxHA7sioiOAecFksQo\n6YtpN14r+yqByi6sypgHOgr4y/K/RfrvcSRJMl4LfJIkEW2V9MNyl5tNHU4aVnMkvZzky/MOYDuw\nFzg5Ihakj/kVM542kHT9DCkitkTEH0fE4SRdK98oj2NU2M6+iqTs+UDLGD7OkQPa2pQ+31R5Hklz\nSbp7WoDyF/6civc+L+P5NgML0/Yqz1v2buBs4I0k3XBHl0OoOGaopa83AP9S8W+xICLmRMQPACLi\n+xHxmvSzBfCljHFbnXDSsJoh6UBJbyXp2/9eRDycTrv9FvAVSYekxy2R9Ob0bRcCH5T0BkkN6b4X\nVWn7XZKOSF/uIvlC6zelN+3quRz4F0nzJB0FfAoY9TUOwEckHSHpIOBvgPIA/A/SuE+VNJOky+3e\ntEtnG0ny+MO0MvgQGRJj+hnWAyuBL0iaIek1wNsqDplH0p20gyQp/esIP8+3gD+V9OtKzJX02+nf\n1wmSfjP9PF0kyX5M06at9jhpWC24VlIbya/YvwG+TP/B578G1gL3pF0qK4AToG/Q/IPAV4A9wG30\nrxTKXg7cK6kduAb4RESsq3Lcx0h+6a8jqXS+TzLwO1rfJxmAX0fSjfbPadwrgL8DriKpDo5l3zgN\nwB+TDO7vAE4G7hrBOd9NMlC+E/h7kokCZZeQdFe1kFwTc89IPkxErExj+xpJ8l0LfCDdPZNk0sJ2\nku6zQ0jGamwKkW/CZGZmWbnSMDOzzJw0zMwsMycNMzPLzEnDzMwyc9KwKUXDLHOu2lmu/PlprBO+\nIGB5raqJPo9ND04aNqVExAHlqbQDF9obqfH8sh2YrCLi2TTW3vFof7xojPfesKnPScPMzDJz0rCa\nJ+mDkq6teP0rSVdUvN4g6dT0eShZBv08kkUBP512A1Uu2neqpIfSpcF/pIrlyivaPBE4n2QRwfZ0\njaUhl2mXtFjSz9I1mXZKuj29Sv1SkqU8rk3b+rQGLH+uZOnzf5J0p5JlxW+UtLginvcpWap9h6S/\nG6qbTdIiJUutt0q6jwFXk0v6n/TvrFXSA5LOSLefCXwO+IM0zgcr/v4fS+NaJ+lPsv7b2dTjpGH1\n4DbgjPQL+HBgBvBKgHT84gDgoco3RMQFJEt+/3vaDVS5lMbvA2cCLwBOYd8VzZXvfwz4U+Du9P0L\n0l2DLtMO/CXJku4Hk6x8+7mkqXgv8CzJzaYOiIh/H+Rzvpvk6vZD0s/4V+lnPAn4BkkSPIxkzaih\nFjb8OskyHocBH0ofle5P4z+I5Ir1KyTNioifkywr8qM0zpemx28F3gocmMb3FUm/NsT5bQpz0rCa\nl45RtJF80f0GcAOwKV1j6rXA7SO8Nez/RsSmdLnya9N2hyUNu0x7geSL+qiIKETE7TGyJRe+ExFP\nRsRekjWwynG9E7g2Iu6IiB6SJFW13XRg/feAz0dER0Q8QrJ0ep+I+F5E7IiIYkT8F8nyHycMFlRE\nXBcRT0XiNpJlUc4Y7Hib2pw0rF7cBryOJGncBtxKkjBem74eiS0VzzsZemnzSsMt0/4fJGsx3Zh2\n43xmnOI6nIrlytOl0HcMEWMT+y/J3kfSX6XdTXvSzzCfAXf3G3D8WZLuSbvcdgNvGep4m9qcNKxe\nlJPGGenz2xg+aYx1YbWB7x9ymfb0xkZ/GRHHAG8HPqV9tzsdSyybSe42CEA6hrJokGO3AUX2X5K9\n/N4zSO4t8vvAwrTbbQ/7lkbvF2e6Yu1VwH8Ch6bHL6s43qYZJw2rF7cBrwdmR8RG4HaScYlFwC8H\nec9zJLdtHa3ngCMkzQAYbpl2JffOPi7txtpDcie7UkVbo43lSuBtkl6VxvIPDPKlnU7h/THwD5Lm\npOMhlbd7nUeSVLYBTZI+TzJWUfmZj1Z6X3SSsZWZ6fFFJfci/61Rfg6bApw0rC5ExJNAO0myICJa\nSZYbv3OIax0uJLn16G5JPxnFaW8G1gBbJG1Ptw26TDvJfbdXpHHeDXwjIm5J9/0b8LdpLH81kiAi\nYg3Jku0/JKk62kkGp7sHectHSbq2tgDfBb5Tse8Gki61J0m6rbro35VVnpW2Q9KqdNzm4yRjLLtI\nBuuvGUn8NrV4aXSzOiPpAGA3cHxEPJ13PDa9uNIwqwOS3pZ2N80lGV94mH339zabNE4aZvXhbJL7\nim8i6QY7Z4TTec3GhbunzMwsM1caZmaWmZOGmZll1pR3AONt0YEHxpGHHJJ3GGZmdeXBp57aHhEH\nD3fclEsaRx5yCDf+13/mHYaZWV059B2/s374o9w9ZWZmI+CkYWZmmTlpmJlZZk4aZmaWmZOGmZll\n5qRhZmaZOWmYmVlmThpmZpaZk4aZmWXmpGFmZpk5aZiZWWZOGmZmlpmThpmZZeakYWZmmTlpmJlZ\nZk4aZmaWmZOGmZll5qRhZmaZOWmYmVlmThpmZpaZk4aZmWXmpGFmZpk5aZiZWWZOGmZmlpmThpmZ\nZeakYWZmmTlpmJlZZk4aZmaWmZOGmZll5qRhZmaZ5ZY0JM2SdJ+kByWtkfSFKsfMlPQjSWsl3Svp\n6MmP1MzMyvKsNLqB34yIlwKnAmdKesWAYz4M7IqI44CvAF+a5BjNzKxCbkkjEu3py+b0EQMOOxu4\nOH1+JfAGSZqkEM3MbIBcxzQkNUpaDWwFlkfEvQMOWQJsAIiIIrAHWFSlnfMkrZS0ckdr60SHbWY2\nbeWaNCKiNyJOBY4ATpf04lG2c0FELI2IpYsOPHB8gzQzsz41MXsqInYDtwBnDtjVAhwJIKkJmA/s\nmNzozMysLM/ZUwdLWpA+nw28CXh8wGHXAO9Pn78TuDkiBo57mJnZJGnK8dyHARdLaiRJXpdHxM8k\n/SOwMiKuAS4ELpW0FtgJnJNfuGZmllvSiIiHgNOqbP98xfMu4F2TGZeZmQ2uJsY0zMysPjhpmJlZ\nZk4aZmaWmZOGmZll5qRhZmaZOWmYmVlmThpmZpaZk4aZmWXmpGFmZpk5aZiZWWZOGmZmlpmThpmZ\nZeakYWZmmTlpmJlZZk4aZmaWmZOGmZll5qRhZmaZOWmYmVlmThpmZpaZk4aZmWXmpGFmZpk5aZiZ\nWWZOGmZmlpmThpmZZeakYWZmmTlpmJlZZk4aZmaWmZOGmZllllvSkHSkpFskPSppjaRPVDnmdZL2\nSFqdPj6fR6xmZpZoyvHcReAvI2KVpHnAA5KWR8SjA467PSLemkN8ZmY2QG6VRkRsjohV6fM24DFg\nSV7xmJnZ8GpiTEPS0cBpwL1Vdr9S0oOSrpd08qQGZmZm/eTZPQWApAOAq4BPRkTrgN2rgKMiol3S\nW4CfAMdXaeM84DyAIw4+eIIjNjObvnKtNCQ1kySMyyLixwP3R0RrRLSnz5cBzZIWVznugohYGhFL\nFx144ITHbWY2XeU5e0rAhcBjEfHlQY55Xnockk4niXfH5EVpZmaV8uyeejXwXuBhSavTbZ8Dng8Q\nEecD7wT+TFIR2AucExGRR7BmNjF6u3rY+vO7Oewdr807FMsgt6QREXcAGuaYrwFfm5yIzCwPz5x/\nFTvvWM2sJQez8OUn5R2ODaMmZk+Z2fRU3NvFzjuSjoZnzr8q52gsCycNM8vN+v+7uu95cU87u+4f\neG2v1RonDTPLRWWVUeZqo/Y5aZhZLiqrjDJXG7XPScPMJl2pp2e/KqPM1UZty/2KcDObfqIEc449\nglJ3z377Zh95aA4RWVZOGmaW2bYV97F71eMc/+n3AdC5fjNPfeX7nPSlj9E4cwalngJr/vqrHPvx\nc5jzgsMHbadx1gxO/tLHqu4rdRd4+JP/xbGfPJc5Rw/ehuXD3VNmlkmpp8CGS5exZ9XjdKxrAWDj\npcvoatnGthvuAWDrjffStXErGy69btTn2br8HrpatrHhkmXjEreNLycNM8tk2/J7iWIv0Vti46XL\n6HxmM62PPg0RbLryJoptnWy6YgVE0Pb4+r7EMhKl7gKbrrgpbeMZOp/eNAGfxMbCScPMhlXqKdBy\n+YpkDCKCtifWs/6CHxOFIgDR28vTX7+CKPYmrwtFNn5v5JXC1uX39Gtjw6WuNmqNk4aZDatcZZRF\noUD7rzZAuhRcqbvA7pWP7hvYHkW1Ua4y+rfhaqPWOGmY2ZD6VRllQV/CGMxIq43KKqOyDVcbtcVJ\nw8yG1LF2A70de0f+xgjaHn2a3q79p9VWs/Ouh/afghtB26Pr6K0yNdfy4Sm3ZjakeScdw9LL/63f\ntiiVkicSkoiIpPJIX1dSQ7bfpif+y58PWr1kbWOydW3eTs/23Rz4kuPyDmXSOGmY2bAGfmnv93o8\nziGBxqOlyfP016+gc/1mTv3239I4c0be4UyK2kzfZmY1rv3J9XSua4HeElt/fnfe4UwaJw0zs1HY\ncMkySoUCpZ4Cm6+6edqMuzhpmJmNUF+VkQ7BRG/vtKk2nDTMzEaoXGWUlbqnT7XhpGFmNgIdT22k\n/fFnoKEBGvc9ert62Lb8vrzDm3CePWVmNgLNBx3IEe99S9V98048enKDyYGThpnZCMxYeCCHnf3a\nvMPIjbunzMwsMycNMzPLzEnDzMwyc9IwM7PMnDTMzCwzJw0zM8sst6Qh6UhJt0h6VNIaSZ+ocowk\n/a+ktZIekvRrecRqNl1suvoWim2dfa933LF6VPf6rkfFtk42/fiWZJl3G1SelUYR+MuIOAl4BfAR\nSScNOOYs4Pj0cR7wzckN0Wz6aH3kKVou+zmbfnwLAMWOvTz99St4+uuXT4sv0s0/uZWW7/+ctjXr\n8g6lpuWWNCJic0SsSp+3AY8BSwYcdjZwSSTuARZIOmySQzWbFjZcch0A2264m0JrB1t+ehsA3Vt2\n0vrQr/IMbcIV2zrYev1dAGy4+GfTIkmOVk2MaUg6GjgNuHfAriXAhorXG9k/sZjZGLU+8hRdLdsA\niAharljBc9fdQRSKlLp72HDJdVP6i3TzT27r+3xdm7a72hhC7klD0gHAVcAnI6J1lG2cJ2mlpJU7\nWkfVhNm0tuGS6/ruzx2FIttuvIdS+ZauTO1qo1xlRKEIkCRJVxuDyjVpSGomSRiXRcSPqxzSAhxZ\n8fqIdFs/EXFBRCyNiKWLDjxwYoI1m6Iqq4w+vSUo9Pa9nMrVRmWVUeZqY3B5zp4ScCHwWER8eZDD\nrgHel86iegWwJyI2T1qQZtPApstXEL29NMyeQcPsGaipsW+fZjUn22fNYO/6LcmS4FNIqbvAc8vu\nRFLf52+YPYPo7aXlR8vzDq8m5bnK7auB9wIPS1qdbvsc8HyAiDgfWAa8BVgLdAIfzCFOsyltybvf\nTM/23X2vC7va6H5uB2pqYs4xh6OGfb8tZy05mGLHXprmzs4j1HGn5kaO/Yt3U+op7LdvxuIFOURU\n+3JLGhFxB6BhjgngI5MTkdn0NO9FR2c+9tmLr2PPLx/nxV/5FElnQX1TQwMLTz857zDqSu4D4WZW\nHwqtHWy94W66t+5iz6rH8w7HcuKkYWaZbL76FoggegpsuHhqDorb8Jw0zGxYSZVxT9+01J6de1xt\nTFNOGmY2rHKVUVbq6nG1MU05aZjZkAqt/S9+K+veutPVxjSU55RbM6sDPdt3M/PQg4je0v77tu3K\nISLLk5OG2TRV2N1Gy4+Wc9Qfv6PftRgDzT1mCS/5n7+auDh2tdFyxQqO+qOzB41jy7W/YM4xR3Dg\nycdU3d+zq5VNV9yUfJYpMBW4lrl7ymya2nTlTWxbfi+77luTaxwtV6xg2433sHvlY1X39+xqZeP3\nrueZb1456BjKpsvTNu5/dCJDNTImDUkLJZ0s6RhJTjRmda6wq41tN98PJIsVRmn/rqfJ0LOrle23\nrNwXR5WksOmKFSBR2N1WNSn07Gxl+60PDNmGjZ9BE4Ck+ZI+J+lh4B7g/4DLgfWSrpD0+skK0szG\n16arbuqbDVVs7cit2th0xb44qiWFJKk8QBR7kxlbVZLCpitWDNmGja+hqoYrSe5lcUZEnBARr0lX\nkj0S+CJwtqQPT0qUZjZuylVGpKvY9n0ZT3K1Ua4yojggjoqkUJkQYP+kUK4yhmrDxtegSSMi3hQR\nl0bE7ir7HoiIT0bEhRMbnpmNt8oqoyyPaqOyyiirTAqVVUbZwKQwMKkMbMPGX9YxjVMkvV3S75Yf\nEx2YmY2/UqHIthX3oYZGGufM6ntEb4nN6b3BJyWO7gLbb74fNVaL42YAtt5wN1Eq9dvfOGcW3Vt2\n0LZmHb3dPUO0MXmfZbrRcGWcpIuAU4A1QLl+jYj40ATHNiqnLHleXPcn7807DLNJ17W9nZmL5g45\n5TQi2Lt5D6WKGyyVNR8wk5mLDuh7PfOlr56QOMtxdD7VQqmnZ/84Fh7IrMMWU9jTTlfL1irvFnOP\nPxI1NQ7bhmV36Dt+54GIWDrccVmu03hFRJw0DjFNioZFhzDrQ5/IOwyzSdWzZRtPnvEejvmPT7P4\nHW8c8tgsd8Lo+PZ/j09gg5DE3OOOGPKY5vkH0Dz/gCGPGa4NG39Zksbdkk6KiProJGxshgP8C8Om\nl5ZvfB0i2PClC1l07rtQY+PwbzIbhSxjGpeQJI4nJD0k6WFJD010YGaWTc/m59h++XXQW6K3rY0d\nV/8875BsCstSaVxIeltW9o1pmFmNaPmP8yGdLlvq2MuGL/w3i37nTFcbNiGyVBrbIuKaiHg6ItaX\nHxMemZkNq1xlRGHfPa5dbdhEylJp/FLS94Frge7yxoj48YRFZWaZtPzH+USx/5LlrjZsImVJGrNJ\nksVvVWwLwEnDLGdNBy1k3itO229747wDKHX30Dgny1wps+yGTRoR8cHJCMTMRu7Iv/145mOLPXsp\nFrqZNXfBBEY0MZ69+GfMO/EFLDz95LxDmfaGHdOQdLGkBRWvF6YX/JlZnYgIutp3U+jqoNRbHP4N\nNaRryw62LruT9d+6mujd/6JEm1xZBsJPqVx/KiJ2AfvXw2ZWs4o9e/vWa+ru3JNzNCPT8oMbiIDe\nvd3suH113uFMe1mSRoOkheUXkg7Cd/wzqxsRQXfHHpKhSCj2dNVNtdG1ZUeykGKpRKmrh42XXe9q\nI2dZksZ/kVzc90+S/gm4C/j3iQ3LzMZLZZVRVi/VRssPbuh3b3JXG/kbNmlExCXA7wLPpY/fjYhL\nJzowMxu7gVVGWT1UG5VVRpmrjfwN2s0k6YCIaAdI153ab+2pymPMrPb0FnuIKAEDV74Nero6mDV3\nfh5hZbL9pvuI3l4aZs3ot72wq43WNeuYf8rxOUU2vQ01NvFTSauBnwIPREQHgKRjgNcDvw98i+QO\nf2aWs47d25gxdwHNzc0A9BYLqKGRuQsOrXq8GvbvaCgWumlsmlHl6Ml3+DvfyOI3nL7/DsHMQw6a\n/IAMGCJpRMQbJL0F+BPg1ekAeAF4ArgOeH9EbBnLydOpu28FtkbEi6vsfx1J0no63fTjiPjHsZzT\nbCrqat9NqbeHrtatNC9aktw3o3U7DU3NzDkw26rPpd4ie1u3M3NObVQfDTObmfW8RXmHYQMMOQsq\nIpYByybw/N8Fvkayku5gbo+It05gDGZ1r9Ddse95oUD09hAR9Ba66S0WaGxqHraN8uB4997WqlWI\nGWS83etEiYhfADvzjMGs3nW17+7/unUrPZ2tlAe/s8yUKvUWKfZ0JS8C4tgXjXeYNkXUw8+JV0p6\nUNL1kqquISDpPEkrJa3csX3bZMdnlqvKKqMsGfxOlKuNofRPLAGnnk4McdtYm75qPWmsAo6KiJcC\nXwV+Uu2giLggIpZGxNJFiw+e1ADN8jSwyhjMUNVGvyqjrLGRwoKF1d9g01qmpCGpUdLhkp5ffkx0\nYAAR0Vox7XcZ0CzJ93I1S1WrMqoZqtqomlCaZ9Bz6KEDru4wy7AciKSPAX9PcmFfueYN4JQJjKt8\n7ucBz0VESDqdJMntmOjzmk0FapxBY1Pyv7gADdLd1Ng0E9T/92NxzQM0LTwYJAinDtsnyxpSnwBO\niIhx/7KW9APgdcBiSRtJklMzQEScD7wT+DNJRWAvcE4MXA/BbBqbt2jJoPv2tu2ksbGZGbMPAKCn\nq4Pezj3Mntd/Gmt5f6WOu29l1mmvGd9gbUrIkjQ2ABOyUE1EnDvM/q+RTMk1sxHoLRbS+2d00Txr\nLpB2Q0XQW+ypmQv4rP4MtYzIp9Kn64BbJV1H/9u9fnmCYzOzUeobp4igp6sdob7lp7o7WzNf8Gc2\n0FCVxrz0z2fTx4z0AQNXPzOzmtFbLNBb6Pt9R8/etn7jEr2FHlcbNmpDLSPyBQBJ74qIKyr3SXrX\nRAdmZqOz32yo/YYBw9WGjVqWKbefzbjNzHI2sMoY9Li02jAbqaHGNM4C3gIskfS/FbsOBGp7IX6z\naarYszfjkUGhe6+7qGzEhhrT2AQ8ALw9/bOsDfiLiQzKrFZFBMWevTTPnDOubRS6O2maMQtpbIs0\nzJg9jxmzk+HIYqGbhoZG1NAIDH6dhtlIDDWm8SDwoKTLImLohWvMpomerg56OveghkaammeOqY2G\nhkYam2fSWyzQ1b6LGbPnMXPOgWOKr5wYIoKu9p00NDYzd76X1rHxM1T31MOks6Sq/UKJiAm/Itys\nlkQEPXtbgWSwuWn+IWNqo6uzlbnzD+4buO7Z286M2QeMudoA6OlqhwhKxQK9hR4am90NZeNjqO6p\n8j0sPpL+Wb4v+B/iKbc2DfV0dfT9l18qFigWukdcbQxso9DVMWB6bPuYq40kMbWVX9HVucfVho2b\nobqn1gNIelNEnFax668lrQI+M9HBmdWKfRXCvt9LI6029m8j6OrovyT5eFQb5SqjzNWGjacs/2VK\n0qsrXrwq4/vMpozKCqGsXG2MpY1qRXvP3vaRB1hurV+Vse8cXRluxGSWRZa1pz4MXCRpPslimbuA\nD01oVGY1pFqVUdbd2UpThq6fodoYcGRabcwb1WynQldH1VVpS0VfBW7jY9ikEREPAC9NkwYR4Z8s\nNu3MnH0g1RZYbmhsHHUbpd4CUSqhBtHQuO/LPFmNPNLnI0scjc0zmDG7+piI1EhEeOqtjclQs6f+\nMCK+V7FwYXk74AULbfqQVHX58Ilso7N1O41NM0Y8KN7YNGPQaqJ7bxu9PV3M8aC4jcFQlcbc9M95\nQxxjZuOst9iT3Gmv0MOMWQeghrEPIUaU6OlsA2JUs77MyoaaPfV/6dMvRUTXYMeZ2fjq7ptRlSxr\nPtYpuNB/cL27Yw9NC0Z+jYkZZBsIf0TSc8Dt6eMOj2uYTYzeYv+FBHv2to+52ogopUkjGScp9RZd\nbdioDftfYkQcB5wLPAz8NsnSIqsnOjCz6ai7Y+DvsaTaGIv9p/BGlfOYZTNs0pB0BPBq4AzgNGAN\n8KMJjsts2hlYZZT17G0nSqVRtTmwyigr9Y7sGhOzsizdU88C9wP/GhF/OsHxmE1byZd49emwvb0F\nmhpG3p1UKpbvYrB/u73uorJRyJI0TgNeA7xb0meAXwG3RcSFExqZWZ0qdHfS0Ng04gvpZs6ex8zZ\n4ztZsbF5BvMWHT6ubdr0luXivgclPQU8RdJF9YfAawEnDbMBolSiq303DY2NzJl/iC+ksyln2KQh\naSUwE7iLZPbUb5QXMzSz/sqD1qXe3qT7Z8asnCMyG19ZuqfOiohtEx6JWZ2LUv9B5+7OPTQ2z3S1\nYVNKlim3ThhmGQycGluuNsymEi9xbjYOBlYZ6Va6O/dUXejQrF45aZiNg57uDpKEoX6PUm+x6rUX\nZvVqqFVuf3eoN0bEj8c/HLOc2wnTAAAVg0lEQVTJlSxNPvbfTs0zZtPQUH2Z9MbG/f83G6/zmk22\noQbC3zbEvgDGnDQkXURyL/KtEfHiKvsF/A/wFqAT+EBErBrrec0g+eJu37WFWfMW0jxj9pjaamhs\noqFKcqimWOhmb+t25i583qCJxqxWDbXK7Qcn4fzfBb4GXDLI/rOA49PHrwPfTP80G7Nk4DpZh6mp\nedakzHKK2LfuU09nG7MOWDDh5zQbT5l+Gkn6beBkoG/SeUT841hPHhG/kHT0EIecDVwSyUjiPZIW\nSDosIjaP9dw2ve0buE6eFwtdY642sugt9lDqTZb2KHR3MGPOPFcbVleyLFh4PvAHwMdIRvfeBRw1\nwXGVLQE2VLzemG7rR9J5klZKWrlju2cI2/DKVUYi+fU/0bOc9lUZ+86T3BjJrH5kGYl7VUS8D9gV\nEV8AXgm8cGLDGpmIuCAilkbE0kWLfStLG1pllVG5rViY2HuNVVYZZYXuDkql3gk9r9l4ypI09qZ/\ndko6HCgAh01cSP20AEdWvD4i3WY2av2rjLKJrTaqVRl98bjasDqSJWn8TNIC4D+AVcAzwA8mMqgK\n1wDvU+IVwB6PZ9hYlUq9qKFx/4cEE9pFFVXPG+FKw+pHloHwf4+IbuAqST8jGQwflzpe0g+A1wGL\nJW0E/h5oBoiI84FlJNNt15JMuZ2MGV02xc0+YOGkn1MScxccOunnNRtvWZLG3cCvAaTJo1vSqvK2\nsYiIc4fZH8BHxnoeMzMbH0NdEf48kplKsyWdxr5bfx0IzJmE2MzMrMYMVWm8GfgAyeDzlyu2twKf\nm8CYzMysRg11RfjFwMWSfi8irprEmMzMrEZlmT11p6QLJV0PIOkkSR+e4LjMzKwGZUka3wFuAMp3\np38S+OSERWRmZjUrS9JYHBGXAyWAiCgCnlhuZjYNZUkaHZIWkV7KWr7IbkKjMjOzmpTlOo1PkVyZ\nfaykO4GDgXdOaFRmlrvi+ifzDsFq0LBJIyJWSXotcALJtRpPRERhwiMbpfa9wV2P1mx4ZnXhpNe9\nneYtThq2v2GThqRZwJ8DryHporpd0vkRMbFLgo5SA8GsXg+5mI3FusZT2DT3tLzDsEn1x5mOytI9\ndQnQBnw1ff1u4FKS+2qY2RR1+AL/+LL9ZUkaL46Ikype3yLp0YkKyMzMaleW2VOr0hlTAEj6dWDl\nxIVkZma1Kkul8TLgLknPpq+fDzwh6WGShWhPmbDozMyspmRJGmdOeBRmZlYXsky5XT8ZgZiZWe3L\nMqZhZmYGOGmYmdkIOGmYmVlmThpmZpaZk4aZmWXmpGFmZpk5aZiZWWZOGmZmlpmThpmZZeakYWZm\nmTlpmJlZZk4aZmaWWa5JQ9KZkp6QtFbSZ6rs/4CkbZJWp48/yiPOqaJUCr7zvV+xaXNn3qGYWZ3K\nLWlIagS+DpwFnAScK+mkKof+KCJOTR/fntQgp5hHHttNy+ZObrxlU96hmFmdyrPSOB1YGxHrIqIH\n+CFwdo7xTGmlUnDzbZuJgC1bOmnZ1JF3SGZWh/JMGkuADRWvN6bbBvo9SQ9JulLSkZMT2tTzyGO7\n6eruBaBQDJbfujnniMysHtX6QPi1wNHpLWWXAxdXO0jSeZJWSlq5Z/f2SQ2wHpSrjEKh1LfN1YaZ\njUaeSaMFqKwcjki39YmIHRHRnb78Nsn9yvcTERdExNKIWDp/weIJCbaeVVYZZa42zGw08kwa9wPH\nS3qBpBnAOcA1lQdIOqzi5duBxyYxvinjoUd2UiyWaGig32NjSwedncW8wzOzOjLsPcInSkQUJX0U\nuAFoBC6KiDWS/hFYGRHXAB+X9HagCOwEPpBXvPVgy3OdrFvfzqtOP6Tf9nPfeQy9vUnX1JNPtXL0\nkXOZMaMRSTQ313oPpZnVktySBkBELAOWDdj2+YrnnwU+O9lx1asfXvU0be1FTjphAQvmz+jb3tgo\nGhsb2b6ji6uvfZYzXnUIr3vNYUO0ZGZWnX9mThFPPd1GW3vS1fSTn62veswtt29Bgnvu377fGIeZ\nWRZOGlPEtdc/2/d8Q0snu/f09Nu/fUcXa9e1EgERwT33b53sEM1sCnDSmAIqq4yygdXGLbdvobc3\nACgWw9WGmY2Kk8YUUFlllFVWG5VVRpmrDTMbDSeNOvfMs/tXGWXXLEuSya13bKFYjH77isXg7vu2\n0dPjasPMsst19pSNTEQgqd/rOXOaOPr5BxAR+x3/wmPnA3DyixZw8OJZ+7XR1Kh+7ZmZDcdJo04U\niyW+eeETvPXMI3jBUfMAuPKn61l00Ezee86xQ773xBMWcOIJ1dswMxsJd0/VidUP72RPaw833ryJ\niGDr9i7WPtXKvSu30bk321Xdqx9K2lietmFmNlJOGnWgWCxx6+1biIBdu3t4en07N9+2md5S8sV/\n1z3DD2gXiyVuvSNpY2fahpnZSDlp1IHVD++kmE6XLRRKXL98I0+vbyMiGdC+/5fbh602Vj/Uv40V\nt7jaMLORc9KoceUqo3JZ8127e/quuSgbqtooVxmVbbjaMLPRcNKocasfTlaorZRc1b3v9XDVxuqH\ndlIY0IarDTMbDSeNGrfumaQbqqlR6RTZffsa021NjcnGlk2d1dtY3wYVbZQfu1t72LvX12mYWXae\nclujOvcW+el1z/IHv3s0DQ1Jbl/5y+2cdMIC5sxJ/tku/v6vWHTQLN56ZnIvqzvveY4Vt7bwZx8+\nEUiqiZW/3M673nF03/UYA9sYaMfObjY/18mLT1w40R/RzOqQK40ade31G1i7ro3b7nwOgM3PdXL9\n8hZ+cdcWAHbs6uLZjZ388qGdFItJt9TNv9jC9h09PPrkLgAeWL2DFbduZu26tgFtPDfoea9fvpGf\nXvcsHR2Fifx4ZlannDRqUOfeIk+ubQXgrnu3UiqVuCm9NesvH9pJe0eB7162tu/4713+DMuWb+h7\n/ZNrn6VQKPUlmOXp2MW+NnZUTQqbn+tkQ0sHkrj9bq9LZWb7c9KoQddevy8BlEpw3Y0b2dDSASQD\n4CtuaaGzc99YxIaNHTzwy519r3t74bobN1BKr+NobStw36rt/dqolhRuunUzxWLQ2xuDJhYzm96c\nNGpMZZVRtvqhXX0LDvb2Bg8/umfYdh5es5tCYd91GbfctrlfGwOTQrnKKBsssZjZ9OakUWMqq4zx\nVBiwyu3ApFCuMspcbZhZNU4aNaRUKu1XZVRqbh75irTNzf2n6TY3K9lGMlDe1d3Ltu1dPL2+naYm\n9e1vbhalUnDPym2j+CRmNlV5ym0NaWho4PSXLe53R72OjgLd3SXmzGnkhcfN51dP7WHDxuR6jOYZ\nDfQWS3Sk4xuzZzXQ3NxAsRhEwMEHz+QlJx3Ejp3ddHQWmTWzkUMOntXXdmNjA81NDcyb18zbzjqy\n6oV+hz1vzgR/ajOrJ04aNebNb1gy5P7TTlnU9/ziH6xl0cKZfddpXHb5UzzzbDuf/dRL+q7tyKKx\nsZFTX3LQ6AI2s2nF3VN1qmVTB5s2d/Lwml20thVob+9h3TPtlEpw022b8w7PzKYoJ406tSIduI4I\nfnHnFn5y3b4B9HtXbqdUKg3xbjOz0XHSqEMtmzrYtCUZ1+gtwYOP7Oy3Ym2Eqw0zmxhOGnVoxYDp\nsdWKClcbZjYRnDTqTMumDjYOspptJVcbZjYRPHuqzmzf0c3cihVqi8USe7v2TdGtvCbjua1dkxma\nmU0DuSYNSWcC/wM0At+OiC8O2D8TuAR4GbAD+IOIeGay46wlL33JQbzU02PNLCe5dU9JagS+DpwF\nnAScK+mkAYd9GNgVEccBXwG+NLlRmplZpTzHNE4H1kbEuojoAX4InD3gmLOBi9PnVwJvkDTytTTM\nzGxc5Jk0lgCVq/NtTLdVPSYiisAeYBFmZpaLKTF7StJ5klZKWrln9/a8wzEzm7LyTBotwJEVr49I\nt1U9RlITMJ9kQLyfiLggIpZGxNL5CxZPULhmZpZn0rgfOF7SCyTNAM4BrhlwzDXA+9Pn7wRujmpL\nsZqZ2aTIbcptRBQlfRS4gWTK7UURsUbSPwIrI+Ia4ELgUklrgZ0kicXMzHKS63UaEbEMWDZg2+cr\nnncB75rsuMzMrLopMRBuZmaTw0nDzMwyc9IwM7PMnDTMzCwzJw0zM8vMScPMzDJz0jAzs8ycNMzM\nLDMnDTMzy8xJw8zMMnPSMDOzzJw0zMwsMycNMzPLzEnDzMwyc9IwM7PMnDTMzCwzJw0zM8vMScPM\nzDJz0jAzs8ycNMzMLDMnDTMzy8xJw8zMMnPSMDOzzJw0zMwsMycNMzPLzEnDzMwyc9IwM7PMnDTM\nzCwzJw0zM8ssl6Qh6SBJyyX9Kv1z4SDH9UpanT6umew4zcysv7wqjc8AN0XE8cBN6etq9kbEqenj\n7ZMXnpmZVZNX0jgbuDh9fjHwjpziMDOzEcgraRwaEZvT51uAQwc5bpaklZLukTRoYpF0Xnrcyj27\nt497sGZmlmiaqIYlrQCeV2XX31S+iIiQFIM0c1REtEg6BrhZ0sMR8dTAgyLiAuACgBe+6LTB2jIz\nszGasKQREW8cbJ+k5yQdFhGbJR0GbB2kjZb0z3WSbgVOA/ZLGmZmNjny6p66Bnh/+vz9wE8HHiBp\noaSZ6fPFwKuBRyctQjMz209eSeOLwJsk/Qp4Y/oaSUslfTs95kRgpaQHgVuAL0aEk4aZWY4mrHtq\nKBGxA3hDle0rgT9Kn98FvGSSQzMzsyH4inAzM8vMScPMzDJz0jAzs8ycNMzMLDMnDTMzy8xJw8zM\nMnPSMDOzzJw0zMwsMycNMzPLzEnDzMwyc9IwM7PMnDTMzCwzJw0zM8vMScPMzDJz0jAzs8ycNMzM\nLDMnDTMzy8xJw8zMMnPSMDOzzJw0zMwsMycNMzPLzEnDzMwyc9IwM7PMnDTMzCwzRUTeMYwrSW3A\nE3nHkdFiYHveQWRQL3FC/cRaL3FC/cRaL3FCbcZ6VEQcPNxBTZMRySR7IiKW5h1EFpJW1kOs9RIn\n1E+s9RIn1E+s9RIn1FesA7l7yszMMnPSMDOzzKZi0rgg7wBGoF5irZc4oX5irZc4oX5irZc4ob5i\n7WfKDYSbmdnEmYqVhpmZTZAplTQknSnpCUlrJX0m73gGI+kiSVslPZJ3LEORdKSkWyQ9KmmNpE/k\nHdNgJM2SdJ+kB9NYv5B3TEOR1Cjpl5J+lncsQ5H0jKSHJa2WtDLveAYjaYGkKyU9LukxSa/MO6aB\nJJ2Q/j2WH62SPpl3XCM1ZbqnJDUCTwJvAjYC9wPnRsSjuQZWhaTfANqBSyLixXnHMxhJhwGHRcQq\nSfOAB4B31OjfqYC5EdEuqRm4A/hERNyTc2hVSfoUsBQ4MCLemnc8g5H0DLA0ImrtmoJ+JF0M3B4R\n35Y0A5gTEbvzjmsw6fdVC/DrEbE+73hGYipVGqcDayNiXUT0AD8Ezs45pqoi4hfAzrzjGE5EbI6I\nVenzNuAxYEm+UVUXifb0ZXP6qMlfRJKOAH4b+HbesUwFkuYDvwFcCBARPbWcMFJvAJ6qt4QBUytp\nLAE2VLzeSI1+wdUjSUcDpwH35hvJ4NIun9XAVmB5RNRqrP8NfBoo5R1IBgHcKOkBSeflHcwgXgBs\nA76Tdvl9W9LcvIMaxjnAD/IOYjSmUtKwCSLpAOAq4JMR0Zp3PIOJiN6IOBU4AjhdUs11/Ul6K7A1\nIh7IO5aMXhMRvwacBXwk7VqtNU3ArwHfjIjTgA6glsc0ZwBvB67IO5bRmEpJowU4suL1Eek2G4N0\nfOAq4LKI+HHe8WSRdk3cApyZdyxVvBp4ezpW8EPgNyV9L9+QBhcRLemfW4GrSbqBa81GYGNFZXkl\nSRKpVWcBqyLiubwDGY2plDTuB46X9II0k58DXJNzTHUtHVy+EHgsIr6cdzxDkXSwpAXp89kkEyIe\nzzeq/UXEZyPiiIg4muS/0Zsj4g9zDqsqSXPTCRCk3T2/BdTcjL+I2AJskHRCuukNQM1N1qhwLnXa\nNQVTaMHCiChK+ihwA9AIXBQRa3IOqypJPwBeByyWtBH4+4i4MN+oqno18F7g4XSsAOBzEbEsx5gG\ncxhwcTorpQG4PCJqejprHTgUuDr57UAT8P2I+Hm+IQ3qY8Bl6Q/GdcAHc46nqjT5vgn4k7xjGa0p\nM+XWzMwm3lTqnjIzswnmpGFmZpk5aZiZWWZOGmZmlpmThpmZZeakYdOSpA9IOjzDcd+V9M6s28ch\nrs9VPD8660rIkj4p6X3jcP6PSvrQWNuxqctJw6arDwDDJo0cfG74Q/qT1AR8CPj+OJz/IpJrHsyq\nctKwupf+In9c0mXpvRSulDQn3fcySbelC+7dIOmwtEJYSnIx2GpJsyV9XtL9kh6RdEF6NXzW8+93\njnT7rZK+lN7n40lJZ6Tb50i6PL1PydWS7pW0VNIXgdlpTJelzTdK+lZ6j5Ab06vdB/pNkmUpimn7\nx0laoeTeIqskHSvpdWmMP5W0TtIXJb0nje1hSccCREQn8IykWlwuxGqAk4ZNFScA34iIE4FW4M/T\ndbO+CrwzIl5G8iv6XyLiSmAl8J6IODUi9gJfi4iXp/c3mQ1kusfFYOeoOKQpIk4HPgn8fbrtz4Fd\nEXES8HfAywAi4jPA3jSm96THHg98PSJOBnYDv1cljFeT3Ouk7LL0PS8FXgVsTre/FPhT4ESSK/1f\nmMb2bfpXFyuBM7J8fpt+pswyIjbtbYiIO9Pn3wM+DvwceDGwPC0cGtn3BTrQ6yV9GpgDHASsAa7N\ncN4ThjlHeZHHB4Cj0+evAf4HICIekfTQEO0/HRHlJVwq26h0GMm9TkjXiloSEVen7Xel2wHuj4jN\n6eungBvT9z8MvL6iva3Ai4aIyaYxJw2bKgauhxOAgDURMeStPyXNAr5Bcoe6DZL+AZiV8bzDnaM7\n/bOX0f3/1l3xvJekChpoL9nirWyrVPG6NCC2WWmbZvtx95RNFc/XvvtCv5vkdq9PAAeXt0tqlnRy\nekwbMC99Xv7C3Z7eO2Qks6KGOsdg7gR+Pz3+JOAlFfsKaZfXSDwGHAd9d1jcKOkdafszy+M7I/BC\nanA1W6sNTho2VTxBcpOgx4CFJDfk6SFJAF+S9CCwmqSPH+C7wPnp6r3dwLdIvihvIFlmP5NhzjGY\nb5AkmkeBfybpCtuT7rsAeKhiIDyL60lud1r2XuDjabfXXcDzRtAWJGMky0f4HpsmvMqt1T0lt6L9\nWTqIXfPS5dubI6IrnbW0AjghTUCjbfNq4NMR8asxxnYa8KmIeO9Y2rGpy2MaZpNvDnBL2g0l4M/H\nkjBSnyEZEB9T0gAWk8zoMqvKlYaZmWXmMQ0zM8vMScPMzDJz0jAzs8ycNMzMLDMnDTMzy8xJw8zM\nMvv/fm6KfhqUuzsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ae29390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the boundaries and the testing data\n",
    "plt.figure(figsize=[6,6])\n",
    "ax = plt.subplot(111)\n",
    "plot_contours(ax, dt, X_train[:, 0], X_train[:, 1],\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.4)\n",
    "plt.scatter(X_test[:,0], X_test[:,1],\n",
    "            marker = '^', c=y_test,\n",
    "            cmap=plt.cm.coolwarm)\n",
    "plt.title('Decision boundaries\\nwith testing data')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see the rectangular decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example: mushroom data\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "The mushroom data you saw earlier is actually part of a large data set of mushrooms. There are 8124 observations with 22 features. In this example, we will use all observations but focus on 3 features that pertain to the cap, namely CapShape, CapSurface, and CapColor, stored in the data file **`mushroom_cap.csv`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<MushroomCapTree.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# loading the data\n",
    "mushroomData = pd.read_csv('mushroom_cap.csv')\n",
    "feature_names = mushroomData.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features in this data set are string variables, and `DecisionTreeClassifier` can only handle numerical categorical variables, we need to transform these features into numerical variables. This is done by the **`LabelEncoder()`** transformation object found in **`sklearn.preprocessing`**. This transforms a string categorical variable into numerical categorical variables. The ordering of the categories can be found by the **`classes_`** attribute of the transformation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Converting string categorical variables to numerical categorical\n",
    "\n",
    "# the target\n",
    "LEEdible = LabelEncoder()\n",
    "y = LEEdible.fit_transform(mushroomData.Edible)\n",
    "Edible_class = LEEdible.classes_\n",
    "# y=0: edible\n",
    "# y=1: poisonous\n",
    "\n",
    "# CapShape\n",
    "LECapShape = LabelEncoder()\n",
    "xCapShape = LECapShape.fit_transform(mushroomData.CapShape)\n",
    "CapShape_class = LECapShape.classes_\n",
    "# xCapShape=0: bell\n",
    "# xCapShape=1: conical\n",
    "# xCapShape=2: convex\n",
    "# xCapShape=3: flat\n",
    "# xCapShape=4: knobbed\n",
    "# xCapShape=5: sunken\n",
    "\n",
    "# CapSurface\n",
    "LECapSurface = LabelEncoder()\n",
    "xCapSurface = LECapSurface.fit_transform(mushroomData.CapSurface)\n",
    "CapSurface_class = LECapSurface.classes_\n",
    "# xCapSurface=0: fibrous\n",
    "# xCapSurface=1: grooves\n",
    "# xCapSurface=2: scaly\n",
    "# xCapSurface=3: smooth\n",
    "\n",
    "# CapColor\n",
    "LECapColor = LabelEncoder()\n",
    "xCapColor = LECapColor.fit_transform(mushroomData.CapColor)\n",
    "CapColor_class = LECapColor.classes_\n",
    "# xCapColor=0: brown\n",
    "# xCapColor=1: buff\n",
    "# xCapColor=2: cinnamon\n",
    "# xCapColor=3: gray\n",
    "# xCapColor=4: green\n",
    "# xCapColor=5: pink\n",
    "# xCapColor=6: purple\n",
    "# xCapColor=7: red\n",
    "# xCapColor=8: white\n",
    "# xCapColor=9: yellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **Mushroom cap decision tree**. Split the data into the training and testing data set, with the testing data set comprising 2000 observations. Generate the decision tree classifier based on the training data set, with **`min_samples_leaf = 9`** and **`max_depth = 7`**. Examine the confusion matrix and classification report on the predicted class labels.\n",
    "2. **Field guide for foragers**. You have some friends who like to gather wild mushrooms when they hike. To keep them from being poisoned, create a table of all possible combinations of cap features (shape, surface, and color), as well as their predicted class (edible or poisonous), so that they can determine the toxicity by looking at the cap. For example, the table may say\n",
    "```\n",
    "Shape        Surface         Color       Edible\n",
    "bell         fibrous         brown       poisonous\n",
    "conical      grooves         buff        edible\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random forest classifier\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "As I mentioned earlier, a decision tree is a realization of infinitely many other possible decision trees to classify the same data. You can use a single tree as a classifier. Or, you can generate a bunch of decision trees to classify the data. That is the idea of a random forest classifier. If you have a collection of decision trees, then a new observation can be classified based on the majority votes on a class from the trees. In some cases, such an ensemble of classifiers may improve the classification performance. \n",
    "\n",
    "Although each tree in a random forest is a decision tree, there are some differences between a tree from a random forest and a regular decision tree.\n",
    "\n",
    "   1. **Features are selected randomly** in a random forest tree. On the other hand, features are selected optimally in a regular decision tree so that entropy is minimized the most.\n",
    "   2. **Training data consists of bootstrap samples** in the random forest algorithm. This means observations are selected randomly, with replacement, from the training data set, to generate a different training data set.\n",
    "   3. **Computationally very intensive** to generate a random forest.\n",
    "   \n",
    "A random forest classifier is implemented as the **`RandomForestClassifier`** object in **`sklearn.ensemble`**. The parameters for the `RandomForestClassifier` is similar to that of a `DecisionTreeClassifier`. Here is an example applied to the mushroom cap data we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<MushroomCapRandomForest.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# loading the data\n",
    "mushroomData = pd.read_csv('mushroom_cap.csv')\n",
    "feature_names = mushroomData.columns[1:]\n",
    "\n",
    "\n",
    "### Converting string categorical variables to numerical categorical\n",
    "\n",
    "# the target\n",
    "LEEdible = LabelEncoder()\n",
    "y = LEEdible.fit_transform(mushroomData.Edible)\n",
    "Edible_class = LEEdible.classes_\n",
    "# y=0: edible\n",
    "# y=1: poisonous\n",
    "\n",
    "# CapShape\n",
    "LECapShape = LabelEncoder()\n",
    "xCapShape = LECapShape.fit_transform(mushroomData.CapShape)\n",
    "CapShape_class = LECapShape.classes_\n",
    "# xCapShape=0: bell\n",
    "# xCapShape=1: conical\n",
    "# xCapShape=2: convex\n",
    "# xCapShape=3: flat\n",
    "# xCapShape=4: knobbed\n",
    "# xCapShape=5: sunken\n",
    "\n",
    "# CapSurface\n",
    "LECapSurface = LabelEncoder()\n",
    "xCapSurface = LECapSurface.fit_transform(mushroomData.CapSurface)\n",
    "CapSurface_class = LECapSurface.classes_\n",
    "# xCapSurface=0: fibrous\n",
    "# xCapSurface=1: grooves\n",
    "# xCapSurface=2: scaly\n",
    "# xCapSurface=3: smooth\n",
    "\n",
    "# CapColor\n",
    "LECapColor = LabelEncoder()\n",
    "xCapColor = LECapColor.fit_transform(mushroomData.CapColor)\n",
    "CapColor_class = LECapColor.classes_\n",
    "# xCapColor=0: brown\n",
    "# xCapColor=1: buff\n",
    "# xCapColor=2: cinnamon\n",
    "# xCapColor=3: gray\n",
    "# xCapColor=4: green\n",
    "# xCapColor=5: pink\n",
    "# xCapColor=6: purple\n",
    "# xCapColor=7: red\n",
    "# xCapColor=8: white\n",
    "# xCapColor=9: yellow\n",
    "\n",
    "\n",
    "\n",
    "# consolidating the features\n",
    "X = np.vstack([xCapShape,xCapSurface,xCapColor]).T\n",
    "\n",
    "# spliting the data into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2000,\n",
    "                                                    random_state=2000)\n",
    "\n",
    "# random forest classifier, training & testing\n",
    "rf = RandomForestClassifier(criterion='entropy',\n",
    "                            n_estimators = 50,\n",
    "                            min_samples_leaf = 9,\n",
    "                            max_depth = 7,\n",
    "                            random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional parameter you have to provide in a random forest classifier is **`n_estimators`**. This is the number of trees to be included in the random forest. In our example, we will go with 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[749 256]\n",
      " [329 666]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     edible       0.69      0.75      0.72      1005\n",
      "  poisonous       0.72      0.67      0.69       995\n",
      "\n",
      "avg / total       0.71      0.71      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=Edible_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better than a regular decision tree? For a comparison, let's take a look at a decision tree on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[753 252]\n",
      " [331 664]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     edible       0.69      0.75      0.72      1005\n",
      "  poisonous       0.72      0.67      0.69       995\n",
      "\n",
      "avg / total       0.71      0.71      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For a comparision, \n",
    "# decision tree classifier, training & testing\n",
    "dt = DecisionTreeClassifier(criterion='entropy', \n",
    "                            min_samples_leaf = 9,\n",
    "                            max_depth = 7,\n",
    "                            random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=Edible_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it doesn't seem like a random forest improves the classifier performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
